{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "#########################################\n",
    "# CS 395C Social Media Behavior\n",
    "# Spring 2023 Semester\n",
    "#----------------------------------------\n",
    "# Artifact Title: Class Demo #5\n",
    "#----------------------------------------\n",
    "# Author: Jeremiah Onaolapo\n",
    "# University of Vermont (UVM)\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c8616",
   "metadata": {},
   "source": [
    "**Note:** This notebook explores the bag-of-words model. It is a vector space representation that allows one to extract features from text (e.g., for machine learning purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238a472",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_file(file_location):\n",
    "    \"\"\"Loads text file from file_location (filename or path)\"\"\"\n",
    "    raw_lines = []\n",
    "    with open(file_location) as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                #print(line)\n",
    "                raw_lines.append(line.strip())\n",
    "    raw_lines = \" \".join(raw_lines)\n",
    "    \n",
    "    return raw_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e2f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_data(s):\n",
    "    \"\"\"Removes punctuation marks. Also sets text to lowercase.\"\"\"\n",
    "    cleaned = s\n",
    "    cleaned = cleaned.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned = cleaned.lower()\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e28995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_dictionary(text_corpus):\n",
    "    \"\"\"Construct a list of all unique tokens that \n",
    "        exist in the corpus (across all documents in it).\n",
    "        NB: Assume that tokens are delimited by \n",
    "        spaces only. Beware of empty/null tokens \n",
    "        (remove them if they exist). Sort the \n",
    "        dictionary alphabetically (in ascending order).\n",
    "        This will return a list of \n",
    "        unique tokens (return type is list, not dict).\"\"\"\n",
    "    \n",
    "    model_dict = []\n",
    "    \n",
    "    uniq_words = set([])\n",
    "    for k, cleaned_text in text_corpus.items():\n",
    "        tokenized = cleaned_text.split(\" \")\n",
    "        tokenized = [t.strip() for t in tokenized if t.strip()]\n",
    "        for elem in tokenized: \n",
    "            uniq_words.add(elem)\n",
    "    \n",
    "    model_dict = list(sorted(uniq_words))\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693de810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document(input_doc, model_dictionary):\n",
    "    \"\"\"Using the input model_dictionary as your reference, \n",
    "    construct a vector representation of the input document. \n",
    "    The resulting vector will be as long as the model_dictionary. \n",
    "    It will encode the presence of each model_dictionary token \n",
    "    with the value 1--or 0 if absent. NB: This will yield \n",
    "    a binary vector, not a count of frequencies.\"\"\"\n",
    "    \n",
    "    doc_vector = []\n",
    "    \n",
    "    temp_table = {}\n",
    "    tokenized = input_doc.split(\" \")\n",
    "    tokenized = [t.strip() for t in tokenized if t.strip()]\n",
    "    \n",
    "    for word in model_dictionary:\n",
    "        if word in tokenized:\n",
    "            # word exists in document\n",
    "            doc_vector.append(1)\n",
    "        else:\n",
    "            # word does not exist in document\n",
    "            doc_vector.append(0)\n",
    "            \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distinguishing_tokens(doc_vectors, model_dictionary):\n",
    "    \"\"\"From the document vectors you constructed, \n",
    "        identify/list all tokens that appear in \n",
    "        the gatsby document but NOT in the alice document.\n",
    "        Also identify/list all tokens that appear in the \n",
    "        alice document but NOT in the gatsby document.\n",
    "        Store the result in the distinguishers dictionary (type dict).\"\"\"\n",
    "\n",
    "    distinguishers = {}\n",
    "\n",
    "    # target schema of distinguishers dict:\n",
    "    #   {\"IN_GATSBY_not_in_alice\": [list of tokens present \n",
    "    #       in the GATSBY document but not in the alice document],\n",
    "    #   \"IN_ALICE_not_in_gatsby\": [list of tokens present \n",
    "    #       in the ALICE document but not in the gatsby document]}\n",
    "\n",
    "    gatsby_vec = []\n",
    "    alice_vec = []\n",
    "    for k, v in doc_vectors.items():\n",
    "        if \"gatsby\" in k:\n",
    "            gatsby_vec = np.array(v)\n",
    "        else:\n",
    "            alice_vec = np.array(v)\n",
    "\n",
    "            \n",
    "    # create a \"mask\" using bitwise XOR of document vectors\n",
    "    xor_of_vectors = np.bitwise_xor(gatsby_vec, alice_vec)\n",
    "    \n",
    "    IN_GATSBY_not_in_alice_vec = list(np.bitwise_and(gatsby_vec, xor_of_vectors))\n",
    "    IN_ALICE_not_in_gatsby_vec = list(np.bitwise_and(alice_vec, xor_of_vectors))\n",
    "\n",
    "    IN_GATSBY_only = [] # to store tokens\n",
    "    IN_ALICE_only = [] # to store tokens\n",
    "    for i in range(0, len(model_dictionary)):\n",
    "        if IN_GATSBY_not_in_alice_vec[i] == 1:\n",
    "            IN_GATSBY_only.append(model_dictionary[i])\n",
    "        if IN_ALICE_not_in_gatsby_vec[i] == 1:\n",
    "            IN_ALICE_only.append(model_dictionary[i])\n",
    "            \n",
    "    # sanity check\n",
    "    print(\"sanity check: this should be an empty set ->\", set(IN_GATSBY_only).intersection(set(IN_ALICE_only)))\n",
    "    print(\"\")\n",
    "    \n",
    "    for k, v in doc_vectors.items():\n",
    "        if \"gatsby\" in k:\n",
    "            distinguishers[\"IN_GATSBY_not_in_alice\"] = IN_GATSBY_only\n",
    "        else:\n",
    "            distinguishers[\"IN_ALICE_not_in_gatsby\"] = IN_ALICE_only\n",
    "    \n",
    "    return distinguishers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5232718",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#METADATA: Source of text corpus (reference information)\n",
    "\n",
    "# Two input files excerpted from the following sources:\n",
    "# 1. \"The Great Gatsby\" (1925) by F. Scott Fitzgerald. Full text available online at https://www.gutenberg.org/ebooks/64317\n",
    "# 2. \"Alice's Adventures in Wonderland\" (1865) by Lewis Carroll. Full text available online at https://www.gutenberg.org/ebooks/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central goal: Construct a basic bag-of-words model over the given corpus\n",
    "\n",
    "input_files = [\"the_great_gatsby_excerpt.txt\", \"alice_in_wonderland_excerpt.txt\"]\n",
    "\n",
    "text_corpus = {}\n",
    "\n",
    "# load and clean input files\n",
    "for elem in input_files:\n",
    "    text_data = load_text_from_file(elem)\n",
    "    cleaned_text = preprocess_text_data(text_data)\n",
    "    # peek inside\n",
    "    print(\"after cleaning {}:\\n     {}...MORE TEXT...\\n\".format(elem, cleaned_text[:80]))\n",
    "    text_corpus[elem.replace(\".txt\", \"\")] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1991ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model dictionary over the entire corpus\n",
    "model_dictionary = construct_model_dictionary(text_corpus)\n",
    "print(\"len(model_dictionary)\", len(model_dictionary))\n",
    "print(model_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct document vectors\n",
    "doc_vectors = {}\n",
    "for label, cleaned_text in text_corpus.items():\n",
    "    # compute the vector representation of cleaned_text (the current document)\n",
    "    current_vec = score_document(cleaned_text, model_dictionary)\n",
    "    # store that vector in the doc_vectors object\n",
    "    doc_vectors[label] = current_vec\n",
    "    print(label, current_vec)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fun task: find the tokens that appear in a document but not the other\n",
    "distinguishers = find_distinguishing_tokens(doc_vectors, model_dictionary)\n",
    "for label, v in distinguishers.items():\n",
    "    print(\"tokens {} -> count = {};\\nselected examples = {}\\n\".format(label, len(v), v[:5]))\n",
    "    #print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
